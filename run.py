# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.
Original file is located at
    https://colab.research.google.com/drive/1G8Emetj07_s0uckM3_KAS6x397Ay-zmC
"""

import torch
import transformers
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch.nn as nn
from huggingface_hub import login
import numpy as np
from copy import deepcopy

# Login to Hugging Face
login('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')

# Load model and tokenizer
llama_13B_path = 'meta-llama/Llama-2-13b-hf'
llama_13B = AutoModelForCausalLM.from_pretrained(llama_13B_path, trust_remote_code=True)
tokenizer = AutoTokenizer.from_pretrained(llama_13B_path, trust_remote_code=True)

# Pruning parameters
INTERVAL = 1
MERGE_LAYERS = 7
HIGHEST_LAY = 39
LOWEST_LAY = 0
THRESHOLD = 0.65

# Define merge function
def merge_layers_return_model(model, merge_base_lay, merge_layer_num):
    merge_layer_num = min(merge_layer_num, len(model.model.layers) - merge_base_lay - 1)
    model_copy = deepcopy(model)

    for diff_lay in range(merge_base_lay + 1, merge_base_lay + 1 + merge_layer_num):
        # MLP Layers
        model_copy.model.layers[merge_base_lay].mlp.gate_proj.weight.data.add_(
            model.model.layers[diff_lay].mlp.gate_proj.weight.data - model_copy.model.layers[merge_base_lay].mlp.gate_proj.weight.data
        )
        model_copy.model.layers[merge_base_lay].mlp.down_proj.weight.data.add_(
            model.model.layers[diff_lay].mlp.down_proj.weight.data - model_copy.model.layers[merge_base_lay].mlp.down_proj.weight.data
        )
        model_copy.model.layers[merge_base_lay].mlp.up_proj.weight.data.add_(
            model.model.layers[diff_lay].mlp.up_proj.weight.data - model_copy.model.layers[merge_base_lay].mlp.up_proj.weight.data
        )
        
        # Attention Layers
        model_copy.model.layers[merge_base_lay].self_attn.q_proj.weight.data.add_(
            model.model.layers[diff_lay].self_attn.q_proj.weight.data - model_copy.model.layers[merge_base_lay].self_attn.q_proj.weight.data
        )
        model_copy.model.layers[merge_base_lay].self_attn.k_proj.weight.data.add_(
            model.model.layers[diff_lay].self_attn.k_proj.weight.data - model_copy.model.layers[merge_base_lay].self_attn.k_proj.weight.data
        )
        model_copy.model.layers[merge_base_lay].self_attn.v_proj.weight.data.add_(
            model.model.layers[diff_lay].self_attn.v_proj.weight.data - model_copy.model.layers[merge_base_lay].self_attn.v_proj.weight.data
        )
        model_copy.model.layers[merge_base_lay].self_attn.o_proj.weight.data.add_(
            model.model.layers[diff_lay].self_attn.o_proj.weight.data - model_copy.model.layers[merge_base_lay].self_attn.o_proj.weight.data
        )

    # Remove merged layers
    for diff_lay in range(merge_base_lay + merge_layer_num, merge_base_lay, -1):
        del model_copy.model.layers[diff_lay]
    
    return model_copy

# Calculate similarity function
def cal_last_hidden_sim(model1, model2, tokenizer, sents):
    sim_ls = []
    for s in sents:
        encoded_inputs = tokenizer(s, return_tensors='pt')
        with torch.no_grad():
            outputs1 = model1(**encoded_inputs, output_hidden_states=True)
            outputs2 = model2(**encoded_inputs, output_hidden_states=True)
        hidden_states1 = outputs1.hidden_states[-1]
        hidden_states2 = outputs2.hidden_states[-1]
        
        sim = torch.cosine_similarity(
            hidden_states1.squeeze(0).flatten().unsqueeze(0),
            hidden_states2.squeeze(0).flatten().unsqueeze(0)
        )
        sim_ls.append(sim.item())
    
    print("Similarities:", sim_ls, "Mean similarity:", np.mean(sim_ls))
    return np.mean(sim_ls)

# Initialize compression
llama_13B_copy_to_compress = deepcopy(llama_13B)
sents = [
    'Mouron () is a commune in the Arde',
    'The 81st Mechanised Brigade () is a mechanised brigade of the Romanian Land Force',
    'There are 18 National Natural Landmarks in the U.S. state of Washington, out of nearly',
    'Torreorgaz is a municipality in the',
    'Copa Libertadores 1973 was won by defending champions Independiente of A'
]

# Pruning process
lay = HIGHEST_LAY - MERGE_LAYERS
while lay >= LOWEST_LAY:
    print("Layer:", lay)
    print("Current model layer count:", len(llama_13B_copy_to_compress.model.layers))
    tmp_merged_model = merge_layers_return_model(llama_13B_copy_to_compress, lay, MERGE_LAYERS - 1)
    sim_value = cal_last_hidden_sim(llama_13B, tmp_merged_model, tokenizer, sents)
    
    if sim_value > THRESHOLD:
        llama_13B_copy_to_compress = tmp_merged_model
        lay -= INTERVAL
        if lay >= len(llama_13B_copy_to_compress.model.layers):
            lay = len(llama_13B_copy_to_compress.model.layers) - 1 - MERGE_LAYERS
    else:
        lay -= 1

llama_13B_copy_to_compress.config.num_hidden_layers = len(llama_13B_copy_to_compress.model.layers)
llama_13B_copy_to_compress
